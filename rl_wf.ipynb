{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Firm Power Provision by Wind Farms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "\n",
    "Wind farms can provide abundent clean, \"free\" energy.\n",
    "\n",
    "Traditional wind farm control aims to maximize the amount of available power.\n",
    "\n",
    "It does this by adjusting the *yaw angle* and the *axial induction factor* of each turbine.\n",
    "\n",
    "But the electricity grid doesn't need *lots* of power, it needs just the right *amount* of power at each point in time.\n",
    "\n",
    "So traditional wind farm control is problematic because:\n",
    "\n",
    "1. power from wind farms is often *curtailed* and let go to waste and \n",
    "2. maximizing the power that the turbines extract from the wind above what is required \n",
    "can lead to unnecessary fatigue loading over time.\n",
    "\n",
    "![Hornsrev Wind Farm](figs/hornsrev.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning is all the rage... but what is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autonomous system learning what to do by **trial and error**:\n",
    "> ... learning what to do — how to map situations to actions — so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. \n",
    "\n",
    "![Reinforcement Learning Feedback Loop](figs/rl.png)\n",
    "\n",
    "where there is a **delayed reward** for actions taken in the present:\n",
    "> ... actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards.\n",
    "\n",
    "![Grid World Example](figs/basicgridworld.png)\n",
    "\n",
    "So the agent fumbles through a **trajectory** of states, actions and rewards and learns as it goes:\n",
    "\n",
    "$S_0,A_0,R_1,S_1,A_1,R_2,S_2,A_2,R_3,\\ldots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Ingredients\n",
    "- Environment, $E$\n",
    "- Agent(s), $A_1, \\ldots, A_l$\n",
    "- States, $s_t \\in \\mathcal{S}$\n",
    "- Observations, $o_t \\in \\mathcal{O}$\n",
    "- Actions, $a_t \\in \\mathcal{A}$\n",
    "- Policy, $\\pi(a_t | s_t)$, probability of selecting action $a_t$ from the current state $s_t$\n",
    "- Reward, $r(s_t, a_t) \\in \\mathbb{R}$\n",
    "\n",
    "At each time-step $t$, the agent tries to select an action $a_t \\sim \\pi(a_t | s_t)$ that maximizes the **discounted return** with a **discount rate** $\\gamma \\in [0, 1]$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G_t &= \\sum_{i=t}^T \\gamma^{(i - t)} r(s_i, a_i)\\\\\n",
    "    &= r(s_t, a_t) + \\sum_{i=t+1}^T \\gamma^{(i - t)} r(s_i, a_i)\\\\\n",
    "    &= r(s_t, a_t) + \\gamma G_{t+1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The **action-value function for policy $\\pi$** describes the expected return after taking an action $a_t$ from state $s_t$ and thereafter following a policy $\\pi$, and is thus is a measure for how good it is to perform a given action in a given state:\n",
    "\n",
    "$$\n",
    "Q^\\pi(s, a) = \\mathbb{E}_\\pi\\left[G_t | s_t=s, a_t=a\\right]\n",
    "$$\n",
    "\n",
    "The action-value function satisfies the recursive relationship known as the **Bellman equation**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q^\\pi(s, a) &= \\mathbb{E}_{r_t, s_{t+1} \\sim E, a_{t+1} \\sim \\pi(s, a)} \\left[G_t | s_t=s, a_t=a\\right]\\\\\n",
    "&= \\mathbb{E}_{r_t, s_{t+1} \\sim E, a_{t+1} \\sim \\pi(s, a)} \\left[r_t + \\gamma G_{t+1} | s_t=s, a_t=a\\right]\\\\\n",
    "&= \\sum_{s',r} \\text{Pr}(s', r | s, a) \\sum_{a'} \\pi(a'|s') \\left[r + \\gamma \\mathbb{E}_{r_{t+1}, s_{t+2} \\sim E, a_{t+2} \\sim \\pi(s, a)} \\left[ G_{t+1} | s_{t+1}=s', a_{t+1}=a'\\right]\\right] \\\\\n",
    "&= \\mathbb{E}_{r_t, s_{t+1} \\sim E} \\left[r_{t} + \\gamma Q^\\pi(s_{t+1}, a_{t+1})\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If the policy can be described as a deterministic function of the state, $\\mu(s)$, then:\n",
    "\n",
    "$$\n",
    "Q^\\mu(s, a) = \\mathbb{E}_{r_t, s_{t+1} \\sim E} \\left[r_{t} + \\gamma Q^\\mu(s_{t+1}, \\mu(s_{t+1}))\\right]\n",
    "$$\n",
    "and the expectation depends only on the environment's influence on $r_t$ and $s_{t+1}$ (and not on the influence of a stochastic policy $\\pi$ on $a_{t+1}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy-Gradient Methods\n",
    "- Learning a parameterized policy, $\\pi(a | s, \\theta)$,\n",
    "- ... by updating the parameters based on the gradient of a performance measure, $J(\\theta)$:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t + \\alpha \\nabla_\\theta \\hat{J}(\\theta_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic Methods \n",
    "Learning a policy (the **actor**) *and* a value function (the **critic**) to assess the action with.\n",
    "- The **policy**, $\\mu(s)$, is the actor.\n",
    "- The **action-value function**, $Q^\\pi(s, a)$, is the critic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Algorithm for Deterministic Policies - Deterministic Policy Gradient (DPG)\n",
    "- learns a deterministic parameterized actor function, $\\mu(s | \\theta^\\mu)$ with a policy-gradient update:\n",
    "$$\n",
    "\\theta^\\mu_{t+1} = \\theta^\\mu_t + \\alpha^\\mu \\nabla_{\\theta^\\mu} \\hat{J}(\\theta^\\mu_t)\n",
    "$$\n",
    "where the **performance measure** is defined as the expected discounted return from the first-time step:\n",
    "$$\n",
    "J = \\mathbb{E}_{r_i, s_i \\sim E, a_i \\sim \\mu} \\left[G_1\\right]\n",
    "$$\n",
    "and the **gradient** with respect to the actor function's parameters is found by appling the chain rule of differentiation:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\theta_\\mu} J &\\approx \\mathbb{E}_{s_t \\sim E} \\left [ \\nabla_{\\theta_\\mu} Q^\\mu(s, a | \\theta^Q) |_{s=s_t,a=\\mu(s_t|\\theta^\\mu)} \\right]\\\\\n",
    "&= \\mathbb{E}_{s_t \\sim E} \\left [ \\nabla_{a} Q^\\mu(s, a | \\theta^Q) |_{s=s_t,a=\\mu(s_t|\\theta^\\mu)} \\nabla_{\\theta^\\mu} \\mu(s|\\theta^\\mu)|_{s=s_t} \\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- learns a parameterized critic function, $Q^\\mu(s, a | \\theta^Q)$ with an estimation error gradient update:\n",
    "\n",
    "$$\n",
    "\\theta^Q_{t+1} = \\theta^Q_{t} + \\alpha^Q \\nabla_{\\theta^Q} Q^\\mu(s, a | \\theta^Q_t) \\left( \\underbrace{r_t + \\gamma Q^\\mu(s_{t+1}, a_t | \\theta^Q)}_\\text{target value} - \\underbrace{Q^\\mu(s_t, a_t | \\theta^Q)}_\\text{current estimate} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Deep Learning into the Mix - Deep Deterministic Policy Gradient (DDPG)\n",
    "Artifical neural networks (ANNs) can be used for nonlinear function approximation.\n",
    "\n",
    "We learn the parameterized critic by minimizing the loss:\n",
    "\n",
    "$$\n",
    "L(\\theta^Q) = \\mathbb{E}\\left[(\\underbrace{y_t}_\\text{target value} - \\underbrace{Q^\\mu(s_t, a_t | \\theta^Q)}_\\text{current estimate})^2\\right]\n",
    "$$\n",
    "\n",
    "where $y_t = r(s_t, a_t) + \\gamma Q^\\mu(s_{t+1}, \\mu(s_{t+1} | \\theta^\\mu))$.\n",
    "\n",
    "This update algorithm is known as **Q-learning**.\n",
    "\n",
    "We learn the parameterized policy by minimizing the loss:\n",
    "\n",
    "$$\n",
    "L(\\theta^\\mu) = \\mathbb{E}\\left[-Q^\\mu(s_t, \\mu(s_t | \\theta^\\mu) | \\theta^Q)\\right]\n",
    "$$.\n",
    "\n",
    "\n",
    "\n",
    "Directly implementing Q-learning with ANNs has proven to be unstable in many environments since the learned critic network $Q^\\mu(s, a | \\theta^Q)$ being updated is also used to calculate the target value $y_t$, so the update is prone to divergence.\n",
    "\n",
    "Enter the **DDPG** solution!\n",
    "1. Create a copy of the actor and critic networks, $\\mu'(s | \\theta^{\\mu'})$ and $Q'(s, a | \\theta^{Q'})$, respectively to calculate the target values.\n",
    "\n",
    "2. Apply \"soft\" updates to the weights of these **target networks**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta^{\\mu'}_{t+1} &= \\tau \\theta^{\\mu}_{t} + (1-\\tau)\\theta^{\\mu'}_{t}\\\\\n",
    "\\theta^{Q'}_{t+1} &= \\tau \\theta^{Q}_{t} + (1-\\tau)\\theta^{Q'}_{t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "where a small value of $\\tau << 1$ means that the target weights are constrained to be updated very slowly, greatly improving the stability of learning at the cost of slower convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To be greedy or learn more - Exploitation vs. Exploration\n",
    "There are **greedy actions**...\n",
    "> If you maintain estimates of the action values, $Q(s, a)$, then at any time step there is at least one action whose estimated value is greatest ... you are *exploiting* your current knowledge of the values of the actions.\n",
    "\n",
    "$$\n",
    "a_t = \\arg\\,\\max\\limits_a Q^\\mu_t(s, a | \\theta^\\mu)\n",
    "$$\n",
    "\n",
    "there are **nongreedy actions**...\n",
    "> If instead you select one of the nongreedy actions ... you are *exploring*, because this enables you to improve your estimate of the nongreedy action’s value.\n",
    "\n",
    "and then there are **$\\epsilon$-greedy** actions...\n",
    "> to behave greedily most of the time, but every once in a while, say with small probability $\\epsilon$, instead select randomly from among all the actions with equal probability\n",
    "\n",
    "$$\n",
    "a_t = \\left\\{ \\begin{matrix}\n",
    "\\text{uniform sample from } \\mathcal{A} \\text{ with probability } \\epsilon \\\\\n",
    "\\arg\\,\\max\\limits_a Q^\\mu_t(s, a | \\theta^\\mu) \\text{ with probability } 1-\\epsilon\n",
    "\\end{matrix} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Too many cooks ... - Multiagent Reinforcement Learning\n",
    "\n",
    "Multiple agents $\\Rightarrow$ multiple state-spaces, observation-spaces, action-spaces, action-value functions, policies $\\Rightarrow$ a *lot* of NNs to keep track of.\n",
    "\n",
    "Enter **Parameter-Sharing**!\n",
    "- Rather than maintaining an actor, critic, target actor and target critic network for every agent, we use one of each for all agents.\n",
    "- Add an **agent indicator** variable to the observation space to distinguish different agents.\n",
    "- Pad shorter observations with zeros to maintain a constant-sized observation space.\n",
    "- Pad shorter actions with zeros to maintain a constant-sized action-space, and truncate when executed for each agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Letting the Wind Farm find its own way in this world ... A Reinforcement-Learning Solution\n",
    "\n",
    "### Objective\n",
    "- Train one agent to adjust the yaw angle of each turbine to coarsely track the power reference \n",
    "without too much yaw travel of turbine loading.\n",
    "- Train another agent to adjust the axial induction factor of each turbine to finely track the power reference.\n",
    "\n",
    "### Constraints\n",
    "- Yaw actuation is *much* slower than axial induction factor actuation\n",
    "- Excessive yaw travel damages the bearings\n",
    "- Excessive thrust on the turbines results in fatigue loading on the components\n",
    "\n",
    "### Autoregressive Observation Space\n",
    "When the yaw angle of a turbine changes, it takes a while for that effect to propagate downstream...\n",
    "\n",
    "So the current reward depends on previous axial induction factors, yaw angles, wind field measurements and online status of each turbine $\\Rightarrow$ **autoregressive observation space**.\n",
    "\n",
    "We also included a preview of the time-varying power reference, the yaw travel for each wind turbine and the thrust force experienced by each turbine.\n",
    "\n",
    "### Action-Spaces\n",
    "Given the set of current and previous observations, $O_t$...\n",
    "\n",
    "For each turbine $i\\in\\mathcal{T}$...\n",
    "\n",
    "every $\\Delta t_\\gamma$ time-steps, choose discrete yaw angle changes $\\gamma^i_t \\in [-1, 0, 1]$...\n",
    "\n",
    "and every $\\Delta t_a$ time-steps, choose continuous axial induction factors $a^i_t \\in [0, \\frac{1}{3}]$\n",
    "\n",
    "### Rewards\n",
    "$$\n",
    "\\begin{aligned}\n",
    "r_\\gamma &= W^P_\\gamma \\exp(-\\beta^P_\\gamma (\\Delta P)^2)\n",
    "\t\t         - W^T_\\gamma * \\exp(-\\beta^T_\\gamma (\\max_i T_i)^2)\n",
    "\t\t         - W^{\\text{Tr}}_\\gamma * \\exp(-\\beta^{\\text{Tr}}_\\gamma (\\max_i \\text{Tr}_i)^2))\\\\\n",
    "r_a &= W^P_a \\exp(-\\beta^P_a (\\Delta P)^2)\n",
    "\\end{aligned}    \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges & Open Questions\n",
    "- The control actions of the yaw angle and axial induction factor agents are **coupled** $\\Rightarrow$ how can we tell which action led to the present reward?\n",
    "- When a turbine goes offline, the dynamics of the wind field in the wind farm can change dramatically $\\Rightarrow$ can the RL algorithm learn from these occurences and adapt?\n",
    "- The choice of weights of the reward functions will greatly influence the results $\\Rightarrow$ how to tune these efficiently for a given wind farm?\n",
    "- We know the power reference we will need to track for the next $24$ hours! $\\Rightarrow$ can we use this to our advantage by learning the best yaw angles for some future time horizon?\n",
    "- Parameter-sharing halves the learning flexibility for the policies and the action-value functions $\\Rightarrow$ will it suffice for our purposes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here goes nothing!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# install packages\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# %cd /content/gdrive/My Drive/rl_wf\n",
    "\n",
    "# ! git clone https://github.com/achenry/rl-fowf-control.git\n",
    "# ! ls\n",
    "# ! cd rl-fowf-control\n",
    "# ! git checkout aoife\n",
    "\n",
    "# ! pip install floris==2.4\n",
    "# ! python rl-fowf-control/floridyn/setup.py develop\n",
    "# ! pip install torch\n",
    "# ! pip install grpcio=1.43.0\n",
    "# ! pip install gymnasium\n",
    "# ! pip install scikit-learn\n",
    "# ! pip install dm_tree\n",
    "# ! pip install gputil\n",
    "# ! pip install pettingzoo\n",
    "print(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-dynfloris",
   "language": "python",
   "name": "rl-dynfloris"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
